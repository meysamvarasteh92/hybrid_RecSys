{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "50b3c6057bc35427fb35c599dc27d9dddacea88a50e08f1b42d82a8da6f2f096"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meysamvarasteh92/hybrid_recsys/blob/main/hybrid_recsys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbpLaBqrJVQ-"
      },
      "source": [
        "import tensorflow as tf\n",
        "from time import time\n",
        "from time import time\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ezPhn97IGja"
      },
      "source": [
        "%%capture\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91dKn6DIh8iL"
      },
      "source": [
        "%%capture\n",
        "all_data = []\n",
        "with open(\"/content/drive/My Drive/dataset.txt\",\"r\",errors=\"ignore\") as f:\n",
        "        all_data = f.readlines()\n",
        "\n",
        "\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=None,\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2: vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data,\n",
        "    3500,\n",
        "    100,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13wseTWHJVim"
      },
      "source": [
        "class parse_args():\n",
        "    path='/content/drive/My Drive/DATASET/'\n",
        "    save_dir='/content/drive/My Drive/DATASET/'\n",
        "    train_ratio=8\n",
        "    valid_ratio=1\n",
        "    test_ratio=1\n",
        "    verbose=10\n",
        "    batch_size=256\n",
        "    max_length=300\n",
        "    vocab_size=8000\n",
        "    num_negatives=4\n",
        "    embed_size_item=200\n",
        "    embed_size_user=100\n",
        "    filter_sizes=[3,4,5]\n",
        "    filter_num=100\n",
        "    middle_dim=200\n",
        "    dropout_keep_prob=0.6\n",
        "    output_dim=50\n",
        "    lr_net=1e-4\n",
        "    train_loss=1\n",
        "    trainable_flag=1\n",
        "    num_epochs=301\n",
        "    weight_size=50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIW3YByBJV1a"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import tensorflow as tf\n",
        "\n",
        "             \n",
        "class Dataset():\n",
        "    \n",
        "    \n",
        "    def __init__(self,args):\n",
        "        self.max_length=args.max_length\n",
        "        self.vocab_size=args.vocab_size\n",
        "        self.embed_size_item=args.embed_size_item\n",
        "        self.batch_size=args.batch_size\n",
        "        self.split_raw_data(args.path+\"ml-1m_ratings.dat\",args.save_dir,args.train_ratio,args.valid_ratio,args.test_ratio)\n",
        "        self.train_tuple_list,self.valid_tuple_list,self.test_tuple_list,self.X_sequence,self.max_len_item,self.max_len_user,self.user_list,self.item_list,self.history,self.history_lab,self.pre_init=self. preprocessing_data(args.path)\n",
        "        self.feed_list_train,self.feed_list_valid,self.feed_list_test=self.generate_batch()\n",
        "        \n",
        "        \n",
        "    def split_raw_data(self,data_path, save_dir, train_ratio, valid_ratio, test_ratio):\n",
        "        ### split data to training, validation and test set\n",
        "         '''\n",
        "         plain text data in the format of userID::itemID::rating\n",
        "         each user and each item will have at least one instance in the training data\n",
        "         :param save_dir: directory for saving processed data\n",
        "         '''\n",
        "\n",
        "         if not os.path.exists(data_path):\n",
        "             sys.exit('invalid path for loading data')\n",
        "         else:\n",
        "             print( 'start processing raw data')\n",
        "\n",
        "             # process rating and review\n",
        "         all_tuple_list = []\n",
        "         user2item = {}\n",
        "         item2user = {}\n",
        "         user2item2line = {}\n",
        "         with open(data_path, 'r', errors='ignore') as f:\n",
        "              for line in f.readlines():\n",
        "                  content = line.strip().split('::')\n",
        "                  u = content[0]\n",
        "                  i = content[1]\n",
        "                  all_tuple_list.append((u, i))\n",
        "      \n",
        "                  if u in user2item:\n",
        "                      user2item[u].append(i)\n",
        "                  else:\n",
        "                      user2item[u] = [i]\n",
        "                  if i in item2user:\n",
        "                      item2user[i].append(u)\n",
        "                  else:\n",
        "                      item2user[i] = [u]\n",
        "      \n",
        "                  if u in user2item2line:\n",
        "                      user2item2line[u][i] = line\n",
        "                  else:\n",
        "                      user2item2line[u] = {i: line}\n",
        "         f.close()\n",
        "         # split rating data\n",
        "         train_set = set()\n",
        "         for (u, item_list) in user2item.items():\n",
        "             i = random.choice(item_list)\n",
        "             train_set.add((u, i))\n",
        "         for (i, user_list) in item2user.items():\n",
        "              u = random.choice(user_list)\n",
        "              train_set.add((u, i))\n",
        "      \n",
        "         total_num = len(all_tuple_list)\n",
        "         train_num = int(train_ratio / (train_ratio + valid_ratio + test_ratio) * total_num)\n",
        "         valid_num = int(valid_ratio / (train_ratio + valid_ratio + test_ratio) * total_num)\n",
        "      \n",
        "         while len(train_set) < train_num:\n",
        "             train_set.add(random.choice(all_tuple_list))\n",
        "         remains_list = list(set(all_tuple_list) - train_set)\n",
        "      \n",
        "         valid_set = set()\n",
        "         while len(valid_set) < valid_num:\n",
        "              valid_set.add(random.choice(remains_list))\n",
        "         test_set = set(remains_list) - valid_set\n",
        "         def write_to_file(save_path,data_set):\n",
        "             with open(save_path, 'w', encoding='utf-8', errors='ignore') as f:\n",
        "                  for (u, i) in data_set:\n",
        "                      line = user2item2line[u][i].strip()\n",
        "                      content = line.split('::')\n",
        "                      new_content = '::'.join(content[2])\n",
        "                      f.write(u + '::' + i + '::' + new_content + '\\n')\n",
        "             \n",
        "             \n",
        "         # save data\n",
        "         if not os.path.exists(save_dir):\n",
        "             os.makedirs(save_dir)   \n",
        "         print('writing rating data to ' + save_dir)\n",
        "         write_to_file(save_dir + 'train', train_set)\n",
        "         write_to_file(save_dir + 'valid', valid_set)\n",
        "         write_to_file(save_dir + 'test', test_set)\n",
        "\n",
        "    \n",
        "    \n",
        "    def preprocessing_data(self,path):\n",
        "        \n",
        "        ### find the items that have description or comment (IMDB dataset)\n",
        "        def document_id(document_name):\n",
        "            with open (document_name,\"r\") as f:\n",
        "                item_plot_id=set()\n",
        "                all_lines=f.read().splitlines()\n",
        "                for line in all_lines:\n",
        "                    tmp=line.split('::')\n",
        "                    item=tmp[0]\n",
        "                    item_plot_id.add(item)\n",
        "            f.close()\n",
        "            return item_plot_id   \n",
        "        \n",
        "        \n",
        "        ## capture history of data (the items that user has seen)\n",
        "        def history_of_data(self,filename):\n",
        "            with open(filename,'r') as f:\n",
        "                hist={}\n",
        "                hist_lab={}\n",
        "                for line in f.readlines():\n",
        "                    content=line.strip().split('::')\n",
        "                    u=content[0]\n",
        "                    i=content[1]\n",
        "                    r=float(content[2])\n",
        "                    if (i in self.item_plot_id):\n",
        "                        if (self.user_dict[u] in hist ):\n",
        "                            hist[self.user_dict[u]].append(self.item_dict[i])\n",
        "                            hist_lab[self.user_dict[u]].append(r)\n",
        "\n",
        "                        else:\n",
        "                            hist[self.user_dict[u]]=[self.item_dict[i]]\n",
        "                            hist_lab[self.user_dict[u]]=[r]\n",
        "\n",
        "                \n",
        "            return hist,hist_lab\n",
        "                \n",
        "        def user_item_id(self,filename):\n",
        "            \n",
        "            ### id of items that have ratings and document and construct dictionary of users and items \n",
        "            user_set = set()\n",
        "            item_set = set()\n",
        "            with open(filename, 'r', errors='ignore') as f:\n",
        "                for line in f.readlines():\n",
        "                    content = line.strip().split('::')\n",
        "                    user_set.add(content[0])\n",
        "                    if (content[1] in self.item_plot_id):\n",
        "                        item_set.add(content[1])\n",
        "            #max_idx_user=int(max(user_set))\n",
        "            #max_idx_item=int(max(item_set))\n",
        "            user_list=list(user_set)\n",
        "            item_list=list(item_set)\n",
        "            user_dict={ old:new for new,old in enumerate(user_list)}\n",
        "            item_dict={ old:new for new,old in enumerate(item_list)}\n",
        "            return user_dict,item_dict,user_list,item_list\n",
        "        \n",
        "        \n",
        "        def read_from_file(self,filename,max_rating,min_rating):\n",
        "            tuple_list = []\n",
        "            with open(filename, 'r', errors='ignore') as f:\n",
        "                for line in f.readlines():\n",
        "                    content = line.strip().split('::')\n",
        "                    u = content[0]\n",
        "                    i =content[1]\n",
        "                    r = float(content[2])\n",
        "                    if max_rating < r:\n",
        "                        max_rating = r\n",
        "                    if min_rating > r:\n",
        "                        min_rating = r\n",
        "                    if (i in self.item_plot_id):\n",
        "                        u_=self.user_dict[u]\n",
        "                        i_=self.item_dict[i]\n",
        "                        tuple_list.append((u_, i_, r))\n",
        "            return tuple_list, max_rating, min_rating\n",
        "                           \n",
        "    \n",
        "    \n",
        "        \n",
        "                \n",
        "        ### split training, validation and test set \n",
        "        max_rating = -1\n",
        "        min_rating = 1e10\n",
        "        self.item_plot_id=document_id(path+\"ml_plot.dat\")\n",
        "        self.user_dict,self.item_dict,user_list,self.item_list=user_item_id(self,path+\"ml-1m_ratings.dat\")\n",
        "        train_tuple_list,max_r,min_r=read_from_file(self,path+\"train\",max_rating,min_rating)\n",
        "        valid_tuple_list,max_r,min_r=read_from_file(self,path+\"valid\",max_rating,min_rating)\n",
        "        test_tuple_list,max_r,min_r=read_from_file(self,path+\"test\",max_rating,min_rating)\n",
        "        self.history_,self.history_lab_=history_of_data(self,path+\"ml-1m_ratings.dat\")   #### history of label, item and position or location of items \n",
        "        \n",
        "        \n",
        "        def document_plot(self,document_name):\n",
        "        ####### construct document matrix (review of users )\n",
        "            with open(document_name,\"r\") as f:\n",
        "                map_idtoplot = {}\n",
        "                all_lines=f.read().splitlines()\n",
        "                for line in all_lines:\n",
        "                    tmp=line.split('::')\n",
        "                    index=tmp[0]\n",
        "                    if (index in self.item_list):\n",
        "                        i=self.item_dict[index]\n",
        "                        plot=tmp[1].split('|')\n",
        "                        eachid_plot = (' '.join(plot)).split()[:self.max_length]\n",
        "                        map_idtoplot[i] = ' '.join(eachid_plot)\n",
        "                            \n",
        "                Raw_X = [map_idtoplot[i] for i in map_idtoplot.keys()]\n",
        "                vectorizer = TfidfVectorizer(max_df=0.5, stop_words={\n",
        "                                             'english'}, max_features=self.vocab_size)\n",
        "                vectorizer.fit(Raw_X)\n",
        "                vocab=vectorizer.vocabulary_\n",
        "                X_sequence = {}\n",
        "                map_idtoplot_list=[]\n",
        "                for i in map_idtoplot.keys():\n",
        "                    X_sequence[i]=[vocab[word]  for word in map_idtoplot[i].split() if word in vocab]\n",
        "                f.close()\n",
        "            return X_sequence,vocab\n",
        "        \n",
        "        \n",
        "        X_sequence,vocab=document_plot(self,path+\"ml_plot.dat\")\n",
        "\n",
        "        ### pre-init vocab with GLOVE\n",
        "        def pre_init_glove(self,vocab,filename):\n",
        "            embeddings_index = dict()\n",
        "            with open (filename,\"r\",errors = 'ignore', encoding='utf8') as f:\n",
        "                 for i,line in enumerate(f):\n",
        "                     values=line.split()\n",
        "                     word=values[0]\n",
        "                     coefs = np.asarray(values[1:], dtype='float32') \n",
        "                     embeddings_index[word] = coefs\n",
        "                 f.close()\n",
        "                 embedding_matrix = np.zeros((self.vocab_size, self.embed_size_item))\n",
        "                 for word, i in vocab.items():\n",
        "                     embedding_vector = embeddings_index.get(word)\n",
        "                     if embedding_vector is not None:\n",
        "                         embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "            return embedding_matrix\n",
        "        \n",
        "        pre_init=pre_init_glove(self,vocab,path+\"glove.6B.200d.txt\")\n",
        "        \n",
        "        ##### masking of user and items in order to have equal length (vector or matrix)\n",
        "        def masking(self,flows,max_size,choice):\n",
        "            if choice=='item':\n",
        "                max_=max(flows,key=lambda k: len(flows[k]))   \n",
        "                self.max_len_item=len(flows[max_])\n",
        "                for i in flows.keys():\n",
        "                    flows[i]=flows[i]+[str(max_size+1)]*(self.max_len_item-len(flows[i]))\n",
        "                \n",
        "                return flows\n",
        "            \n",
        "            elif choice=='user':\n",
        "                max_=max(flows,key=lambda k: len(flows[k]))  \n",
        "                self.max_len_user=100\n",
        "                for i in flows.keys():\n",
        "                    if(self.max_len_user>len(flows[i])):\n",
        "                        #flows[i]=flows[i]+[str(max_size+1)]*(self.max_len_user-len(flows[i]))\n",
        "                        flows[i]=flows[i]+['0']*(self.max_len_user-len(flows[i]))\n",
        "                    else:\n",
        "                        flows[i]=flows[i][:self.max_len_user]\n",
        "\n",
        "                return flows\n",
        "                \n",
        "               \n",
        "        \n",
        "        \n",
        "        X_sequence_=masking(self,X_sequence,self.vocab_size,'item')\n",
        "        self.history=masking(self,self.history_,int(len(self.item_list)),'user')\n",
        "        self.history_lab=masking(self,self.history_lab_,5,'user')\n",
        "        return train_tuple_list,valid_tuple_list,test_tuple_list,X_sequence_,self.max_len_item,self.max_len_user,user_list,self.item_list,self.history,self.history_lab,pre_init\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        \n",
        "        \n",
        "        def get_train_batch(self):\n",
        "            feed_list_train=[]\n",
        "            num_batches=len(self.train_tuple_list)//self.batch_size\n",
        "            for i in range(num_batches):\n",
        "                dict_train= get_single_train_batch(self,self.train_tuple_list,i)\n",
        "                feed_list_train.append(dict_train)\n",
        "            \n",
        "            return feed_list_train\n",
        "        \n",
        "\n",
        "        def get_single_test_and_valid_batch(self,dataset,l):\n",
        "          user_batch_, item_batch_,raring_batch_=[],[],[]\n",
        "          begin=l*self.batch_size\n",
        "          end=begin+self.batch_size\n",
        "          for u,i,r in dataset[begin:end]:\n",
        "              user_batch_.append(u)\n",
        "              item_batch_.append(i)\n",
        "              raring_batch_.append(r)\n",
        "            ### item section\n",
        "          input_seq_item_test=np.zeros([len(item_batch_),self.max_len_item])\n",
        "          for i_,j_ in enumerate(item_batch_):\n",
        "              input_seq_item_test[i_]=self.X_sequence[j_]\n",
        "            ###user section \n",
        "          input_seq_user_test=np.zeros([len(user_batch_),self.max_len_user])\n",
        "          for ii_,jj_ in enumerate(user_batch_):\n",
        "              input_seq_user_test[ii_]=self.history[jj_]\n",
        "\n",
        "          feed_dict={'user_input':input_seq_user_test_,'item_input':input_seq_item_test_,'label_input':input_seq_label_test_,'label':label}  \n",
        "          return feed_dict\n",
        "      \n",
        "        \n",
        "        def get_test_batch(self):\n",
        "            feed_list_test=[]\n",
        "            num_batches=len(self.test_tuple_list)//self.batch_size\n",
        "            for i in range(num_batches):\n",
        "                dict_test= get_single_test_and_valid_batch(self,self.test_tuple_list,i)\n",
        "                feed_list_test.append(dict_test)\n",
        "            \n",
        "            return feed_list_test\n",
        "        \n",
        "        \n",
        "        def get_valid_batch(self):\n",
        "            feed_list_valid=[]\n",
        "            num_batches=len(self.valid_tuple_list)//self.batch_size\n",
        "            for i in range(num_batches):\n",
        "                dict_valid= get_single_test_and_valid_batch(self,self.valid_tuple_list,i)\n",
        "                feed_list_valid.append(dict_valid)\n",
        "            \n",
        "            return feed_list_valid\n",
        "\n",
        "        feed_list_train=get_train_batch(self)\n",
        "        fee_list_valid=get_valid_batch(self)\n",
        "        fee_list_test=get_test_batch(self)\n",
        "\n",
        "        \n",
        "        return feed_list_train,fee_list_valid,fee_list_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRw4lh-JOJfX"
      },
      "source": [
        "%%capture\n",
        "args = parse_args()\n",
        "dataset=Dataset(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqHQyzZaJxO6"
      },
      "source": [
        "\n",
        "class CMF:\n",
        "    def __init__(self,args,dataset): \n",
        "        self.num_users=len(dataset.user_list)\n",
        "        self.num_items=len(dataset.item_list)\n",
        "        self.vocab_size=args.vocab_size\n",
        "        self.filter_num=args.filter_num\n",
        "\n",
        "    \n",
        "    def _create_placeholders(self):\n",
        "        with tf.name_scope(\"input_data\"):\n",
        "            self.user_input = tf.compat.v1.placeholder(tf.int32, shape=[None, None],name='user_input')\t# the index of users\n",
        "            self.item_input = tf.compat.v1.placeholder(tf.int32, shape=[None, None],name='item_input')\t  #the index of documents\n",
        "            self.label_input = tf.compat.v1.placeholder(tf.int32, shape=[None, None],name='label_input')\t  #the index of labels\n",
        "            self.label=tf.compat.v1.placeholder(tf.float32, shape=[None, 1],name='label')\t  #the index of documents\n",
        "            \n",
        "\n",
        "    def _create_variables(self):  \n",
        "        with tf.name_scope(\"embedding\"):          # The embedding initialization is unknown now\n",
        "            self.document_embedding = tf.Variable(self.pre_init,name='document_embedding', dtype=tf.float32,trainable=True)\n",
        "            self.label_embedding=tf.Variable(tf.random.truncated_normal(shape=[6, self.embedding_size_user], mean=0.0, stddev=0.1),name='label_embedding1', dtype=tf.float32,trainable=True)\n",
        "\n",
        "            ####  Variables for attention\n",
        "            self.W_att = tf.Variable(tf.random.truncated_normal(shape=[self.embedding_size_user*2, self.weight_size], mean=0.0, stddev=tf.sqrt(tf.math.truediv(2.0, self.weight_size + self.embedding_size_user))),name='Weights_for_MLP', dtype=tf.float32, trainable=True)\n",
        "            self.b_att = tf.Variable(tf.random.truncated_normal(shape=[1, self.weight_size], mean=0.0, stddev=tf.sqrt(tf.math.truediv(2.0, self.weight_size + self.embedding_size_user))),name='Bias_for_MLP', dtype=tf.float32, trainable=True)\n",
        "            \n",
        "            self.h_att = tf.Variable(tf.ones([self.weight_size, 1]), name='H_for_MLP', dtype=tf.float32,trainable=True)\n",
        "           \n",
        "   \n",
        "    \n",
        "    def _create_inference(self):\n",
        "        with tf.compat.v1.variable_scope(\"inference\",reuse=tf.compat.v1.AUTO_REUSE):\n",
        "            self.item_look_up=tf.nn.embedding_lookup(self.document_embedding,self.item_input)    ### (batch_size,length_of_the_words,embedding_size)\n",
        "            self.label_look_up=tf.nn.embedding_lookup(self.label_embedding,self.label_input)       ### (batch_size,1,embedding_size)\n",
        "            self.item_look_up=tf.expand_dims(self.item_look_up,-1)                               ## (batch_size,length_of_the_words,embedding_size,1)\n",
        "            \n",
        "            \n",
        "            # convolution\n",
        "\n",
        "            pooled_outputs = []\n",
        "            for filter_size in self.filter_sizes:\n",
        "                with tf.name_scope(name='conv-maxpool-{}'.format(filter_size)):\n",
        "                    filter_shape = [filter_size, self.embedding_size_item, 1, self.filter_num]\n",
        "                    self.W = tf.compat.v1.get_variable(name='conv-W{}'.format(filter_size),shape= filter_shape)  # xavier_initializer by default\n",
        "                    self.b = tf.compat.v1.get_variable(name='conv-b{}'.format(filter_size), shape=[self.filter_num], initializer=tf.constant_initializer(0.1))\n",
        "                    conv = tf.nn.conv2d(input=self.item_look_up, filters=self.W, strides=[1, 1, 1,1], padding='VALID')\n",
        "                    h = tf.nn.relu(conv + self.b)\n",
        "                    pooled = tf.nn.max_pool(input= h, ksize=[1, self.max_seq - filter_size + 1, 1, 1], strides=[1, 1, 1, 1], padding='VALID')  # (batch_size, 1, 1, filter_num)\n",
        "                    pooled_outputs.append(pooled)\n",
        "                 \n",
        "\n",
        "\n",
        "            # concatenate results of max pooling\n",
        "            filter_total_num = self.filter_num * len(self.filter_sizes)\n",
        "            h_pool = tf.concat(values=pooled_outputs, axis=-1)\n",
        "            h_pool_flat = tf.reshape(tensor=h_pool, shape=[-1, filter_total_num])\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            # fully connected layers\n",
        "            hidden_item = tf.compat.v1.layers.dense(inputs=h_pool_flat, units=self.middle_dim, activation=tf.nn.relu,\n",
        "                                 kernel_initializer=tf.compat.v1.truncated_normal_initializer(mean=0.0, stddev=0.1),\n",
        "                                 bias_initializer=tf.constant_initializer(0.1))\n",
        "\n",
        "            dropped_item = tf.nn.dropout(hidden_item, self.dropout_keep_prob)\n",
        "            self.output_vec_item =tf.compat.v1.layers.dense(inputs=dropped_item, units=self.output_dim, activation=tf.nn.relu,\n",
        "                                          kernel_initializer=tf.compat.v1.truncated_normal_initializer(mean=0.0, stddev=0.1),\n",
        "                                         bias_initializer=tf.constant_initializer(0.1))\n",
        "            \n",
        "            \n",
        "            ### fully connected layers for user section\n",
        "            self.output_vec_user=self.bert_end_to_end()\n",
        "            input_MLP=tf.concat([self.output_vec_item,self.output_vec_user],axis=-1)\n",
        "            hidden1=tf.compat.v1.layers.dense(inputs=input_MLP,units=100,activation=tf.nn.relu,\n",
        "                                          kernel_initializer=tf.compat.v1.truncated_normal_initializer(mean=0.0, stddev=0.1),bias_initializer=tf.constant_initializer(0.1))\n",
        "            hidden2=tf.compat.v1.layers.dense(inputs=hidden1,units=50,activation=tf.nn.relu,\n",
        "                                          kernel_initializer=tf.compat.v1.truncated_normal_initializer(mean=0.0, stddev=0.1),bias_initializer=tf.constant_initializer(0.1))\n",
        "            self.output=tf.compat.v1.layers.dense(inputs=hidden2,units=1,activation=tf.nn.relu,\n",
        "                                          kernel_initializer=tf.compat.v1.truncated_normal_initializer(mean=0.0, stddev=0.1),\n",
        "                                        bias_initializer=tf.constant_initializer(0.1))\n",
        "          \n",
        "    def _bert(self):      \n",
        "      mlm_model = keras.models.load_model(\"/content/drive/My Drive/bert_mlm_v2.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel})\n",
        "      pretrained_bert_model = tf.keras.Model(mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output )\n",
        "      return pretrained_bert_model\n",
        "    \n",
        "    def bert_end_to_end(self):\n",
        "      inputs=self.user_input\n",
        "      sequence_output = self._bert()(inputs)\n",
        "      pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
        "      hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
        "      outputs = layers.Dense(50, activation=\"sigmoid\")(hidden_layer)\n",
        "      return outputs\n",
        "    \n",
        "    #### define attention layer\n",
        "    def _attention_MLP(self,q):\n",
        "          with tf.name_scope(\"attention_MLP\"):\n",
        "              b = tf.shape(q)[0]\n",
        "              n = tf.shape(q)[1]\n",
        "              MLP_output = tf.matmul(q, self.W_att) + self.b_att \n",
        "              MLP_output = tf.nn.relu( MLP_output )\n",
        "              A_ = tf.reshape(tf.matmul(MLP_output, self.h_att),[b,n]) \n",
        "              exp_A_=tf.math.exp(A_)\n",
        "              exp_sum=tf.math.reduce_sum(exp_A_,1,keepdims=True)\n",
        "              A = tf.expand_dims(tf.math.truediv(exp_A_, exp_sum),2) # (b, n, 1)\n",
        "              return tf.math.reduce_sum(A * self.user_look_up, 1)   \n",
        "\n",
        "\n",
        "\n",
        "    ### define loss (RMSE)\n",
        "    def _create_loss(self):\n",
        "        with tf.name_scope(\"loss\"):\n",
        "             loss1=tf.keras.losses.MSE(self.label,self.output)\n",
        "             self.loss=(tf.math.sqrt(tf.math.maximum(loss1,1e-8)))\n",
        "                      \n",
        "                                              \n",
        "\n",
        "    ### define optimizer (Adam)\n",
        "          \n",
        "    def _create_optimizer(self):\n",
        "        with tf.name_scope(\"optimizer\"):\n",
        "             self.optimizer=tf.compat.v1.train.AdamOptimizer(self.lr_net).minimize(self.loss)\n",
        "             \n",
        "\n",
        "\n",
        "             \n",
        "    def build_graph(self):\n",
        "       self._create_placeholders()\n",
        "       self._create_variables()\n",
        "       self._create_inference()\n",
        "       self._create_loss()\n",
        "       self._create_optimizer()\n",
        "        \n",
        "\n",
        "\n",
        "#### training network\n",
        "def training(flag,model,data,num_epochs):\n",
        "    \n",
        "    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
        "         sess.run(tf.compat.v1.global_variables_initializer())\n",
        "         ### initialize for training batch\n",
        "         batch_begin = time()\n",
        "         feed_list=data.feed_list_train\n",
        "         feed_list_test=data.feed_list_test\n",
        "         batch_time = time() - batch_begin\n",
        "         ### call  dataset as batch\n",
        "         num_batches_train=len(feed_list)\n",
        "         num_batches_test=len(feed_list_test)\n",
        "         batch_index_train=range(num_batches_train)\n",
        "         batch_index_test=range(num_batches_test)\n",
        "         #### training loop\n",
        "         for epoch_count in range(num_epochs):\n",
        "             train_begin=time()\n",
        "             training_batch(batch_index_train,model,sess,feed_list)\n",
        "             train_time = time() - train_begin\n",
        "             #### evaluate the moddel  with test data\n",
        "             if epoch_count % model.verbose==0:\n",
        "                 if model.train_loss:\n",
        "                    loss_begin = time()\n",
        "                    train_loss=training_loss(batch_index_train,model,sess,feed_list)\n",
        "                    test_loss = testing_loss(batch_index_test,model,sess,feed_list_test)\n",
        "                    loss_time = time() - loss_begin\n",
        "                 else:\n",
        "                    loss_time, train_loss = 0, 0 \n",
        "\n",
        "                 print('epoch:',epoch_count,'loss_test:',np.mean(test_loss),'loss_train:',np.mean(train_loss))\n",
        "                 \n",
        "### training the model for eatch batch in 1 epoch\n",
        "def training_batch(batch_index,model,sess,feed_list):\n",
        "    for idx in (batch_index):\n",
        "        feed_dict_=feed_list[idx]\n",
        "        feed_dict={model.user_input:feed_dict_['user_input'],model.item_input:feed_dict_['item_input'],model.label_input:feed_dict_['label_input'],model.label:feed_dict_['label']}\n",
        "        sess.run([model.loss,model.optimizer],feed_dict)\n",
        "\n",
        "### compute loss of training data\n",
        "def training_loss(batch_index,model,sess,feed_list):\n",
        "    train_loss=0.0\n",
        "    for idx in batch_index:\n",
        "        feed_dict_=feed_list[idx]\n",
        "        feed_dict={model.user_input:feed_dict_['user_input'],model.item_input:feed_dict_['item_input'],model.label_input:feed_dict_['label_input'],model.label:feed_dict_['label']}\n",
        "        train_loss+=sess.run(model.loss,feed_dict)\n",
        "    return train_loss/len(batch_index)\n",
        "\n",
        "\n",
        "### compute loss of test data\n",
        "def testing_loss(batch_index_test,model,sess,feed_list):\n",
        "    test_loss=0.0\n",
        "    for idx in batch_index_test:\n",
        "        feed_dict_=feed_list[idx]\n",
        "        feed_dict={model.user_input:feed_dict_['user_input'],model.item_input:feed_dict_['item_input'],model.label_input:feed_dict_['label_input'],model.label:feed_dict_['label']}\n",
        "        test_loss+=sess.run(model.loss,feed_dict)\n",
        "    return test_loss/len(batch_index_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4-PmNSN9Iv5"
      },
      "source": [
        "if __name__=='__main__': \n",
        "    with tf.device('/GPU:0'):\n",
        "        args = parse_args()\n",
        "        dataset=Dataset(args)\n",
        "        model=CMF(args,dataset)\n",
        "        tf.compat.v1.disable_eager_execution()\n",
        "        model.build_graph()\n",
        "        training(flag=1, model, dataset,args.num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}